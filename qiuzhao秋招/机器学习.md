# 机器学习  
   主要存放一些经典机器学习模型，总结其原理、使用场景、常用考点，比如svm，k-means,马尔科夫链，逻辑回归，朴素贝叶斯，普聚类等。 
## 1. 回归模型

## 2. 分类模型

### 1. 朴素贝叶斯

朴素贝叶斯是贝叶斯分类器的一种。是一种有监督学习。基础是贝叶斯决策论。其中“朴素”一词是因为对待训练的各个特征进行了较强的独立性假设。不考虑特征之间的相关性。

* **先验概率:** 是指全事件为背景下的，事件A发生的概率，P（a|Ω）
* **后验概率** 是指新事件B发生的背景下，事件A发生的概率，p（a|b）

全事件一般是统计获得的，所以称之为先验概率，也就是没有实验前的概率。

新事件一般是实验，如实验b， 此时的事件背景从全时间改为事件b发生，该事件b对事件a的发生是可能有影响的，那么需要对事件a的发生概率进行修正，即从事件P(a|Ω)变为P(a|b)。于是称P(a|b)为后验概率。，也就是事件b发生后的概率。

https://zhuanlan.zhihu.com/p/26464206

**贝叶斯公式：**
$$
p(c|x) = \frac{p(c) p(x|c)}{p(x)}
$$
朴素贝叶斯模型的思路就是根据贝叶斯公式以及给定的数据来计算先验概率p(c)以及似然概率p(x|c) [也就是c发生的情况下，x出现的比重]

**举例说明：**

使用西瓜书中的西瓜数据集3.0来演示一下朴素贝叶斯的整体计算流程。

![img](D:\Data_structure\Data_structure\qiuzhao秋招\picture\西瓜数据集.png)

首先计算先验概率p(c),
$$
P(好瓜= 是) = 8/17 \approx 0.471 \\
P(好瓜 = 否) = 9/17 \approx 0.529
$$
然后计算条件概率：
$$
P_{青绿|是} = P（色泽 = 青绿 | 好瓜 =  是） = 3/8 \approx 0.375 \\
P_{青绿|否} = P（色泽= 青绿|好瓜= 否） = 3/9 \approx 0.333
$$
其他离散属性同理可算。

对于连续属性，比如密度：

好瓜的密度属性均值为前八个密度值求平均。方差为每个属性减去均值的平方和，再求平均。https://www.jianshu.com/p/c1716381cb33

````
haogua = df['密度'][:8] # 取前8个样本，求密度的均值和标准差
mean, std = haogua.mean(), haogua.std()
print("好瓜的密度均值和标准差:\n", mean, std)

langua = df['密度'][8:] # 取前后9个样本，求密度的均值和标准差
mean, std = langua.mean(), langua.std()
print("烂瓜的密度均值和标准差:\n", mean, std)
````

结果为：

![img](D:\Data_structure\Data_structure\qiuzhao秋招\picture\西瓜数据集密度结果.png)

那么条件概率计算如下：
$$
P_{密度= 0.697|是} = P（密度 = 0.679|好瓜 = 是） = \frac{1}{\sqrt{2 \pi} * 0.129}\exp(- \frac{(0.697 - 0.574)^2}{2*0.129^2}) \approx 1.959 \\
P_{密度= 0.697|否} = P（密度 = 0.679|好瓜 = 否） = \frac{1}{\sqrt{2 \pi} * 0.195}\exp(- \frac{(0.697 - 0.496)^2}{2*0.195^2}) \approx 1.203
$$
接着计算一个案例：

![img](D:\Data_structure\Data_structure\qiuzhao秋招\picture\西瓜数据集案例.png)
$$
&& P(好瓜=是) \times P_{青绿|是}\times P_{蜷缩|是}\times P_{浊响|是}\times P_{清晰|是}\times P_{凹陷|是}\times P_{硬滑|是}\times P_{密度=0.697|是}\times P_{含糖=0.460|是} \approx 0.052 \\ 
&& P(好瓜=否) \times P_{青绿|否}\times P_{蜷缩|否}\times P_{浊响|否}\times P_{清晰|否}\times P_{凹陷|否}\times P_{硬滑|否}\times P_{密度=0.697|否}\times P_{含糖=0.460|否} \approx 6.8 \times 10^{-5}
$$
由于好瓜的概率远大于坏瓜，则模型判定其为好瓜。



**拉普拉斯平滑**：

仔细观察上述例子会发现一个问题，若某个属性值在训练集中没有与某个类别同时出现的时候，如果直接使用频率统计来进行概率估计会出现问题。例如，如果某个测试用例中包含了"敲声=清脆"这一属性，那么：
$$
P_{清脆|是} = P（敲声 = 清脆| 好瓜 = 是） = 0/8 = 0
$$
由于极大似然估计使用的是连乘，这就会导致估计出的的条件概率为0，这样显然是不合理的。
 为了避免其他属性携带的信息被训练集中未出现的属性值给抹去，在估计概率值时通常要进行”平滑“（smoothing)操作，常用的但是拉布拉斯修正(Laplacian correction)。具体来说，条件概率的贝叶斯估计为：
$$
P_{\lambda} (x^{j} = a_{jl} | y = c_k) = \frac{\sum_{i=1}^{N} I(x_i^{(j)} = a_{jl}, y = c_k) + \lambda}{\sum_{i=1}^{N} I(y_i = c_k) + \lambda S_{j}}
$$
其中$\lambda \geq 0$ 表示拉普拉斯系数，一般设为1，表示拉普拉斯平滑。$S_{j}$代表第$j$个特征的取值个数（比如青绿的$S_j$就是3，好瓜就是2）。



代码如下：

```python
import pandas as pd
from io import StringIO

data = '编号,色泽,根蒂,敲声,纹理,脐部,触感,密度,含糖率,好瓜\n\
1,青绿,蜷缩,浊响,清晰,凹陷,硬滑,0.697,0.46,是\n\
2,乌黑,蜷缩,沉闷,清晰,凹陷,硬滑,0.774,0.376,是\n\
3,乌黑,蜷缩,浊响,清晰,凹陷,硬滑,0.634,0.264,是\n\
4,青绿,蜷缩,沉闷,清晰,凹陷,硬滑,0.608,0.318,是\n\
5,浅白,蜷缩,浊响,清晰,凹陷,硬滑,0.556,0.215,是\n\
6,青绿,稍蜷,浊响,清晰,稍凹,软粘,0.403,0.237,是\n\
7,乌黑,稍蜷,浊响,稍糊,稍凹,软粘,0.481,0.149,是\n\
8,乌黑,稍蜷,浊响,清晰,稍凹,硬滑,0.437,0.211,是\n\
9,乌黑,稍蜷,沉闷,稍糊,稍凹,硬滑,0.666,0.091,否\n\
10,青绿,硬挺,清脆,清晰,平坦,软粘,0.243,0.267,否\n\
11,浅白,硬挺,清脆,模糊,平坦,硬滑,0.245,0.057,否\n\
12,浅白,蜷缩,浊响,模糊,平坦,软粘,0.343,0.099,否\n\
13,青绿,稍蜷,浊响,稍糊,凹陷,硬滑,0.639,0.161,否\n\
14,浅白,稍蜷,沉闷,稍糊,凹陷,硬滑,0.657,0.198,否\n\
15,乌黑,稍蜷,浊响,清晰,稍凹,软粘,0.36,0.37,否\n\
16,浅白,蜷缩,浊响,模糊,平坦,硬滑,0.593,0.042,否\n\
17,青绿,蜷缩,沉闷,稍糊,稍凹,硬滑,0.719,0.103,否'

df = pd.read_csv(StringIO(data), index_col=0)

from sklearn import preprocessing

le = preprocessing.LabelEncoder()
df['色泽'] = le.fit_transform(df['色泽'].values)
df['根蒂'] = le.fit_transform(df['根蒂'].values)
df['敲声'] = le.fit_transform(df['敲声'].values)
df['纹理'] = le.fit_transform(df['纹理'].values)
df['脐部'] = le.fit_transform(df['脐部'].values)
df['触感'] = le.fit_transform(df['触感'].values)
df['好瓜'] = le.fit_transform(df['好瓜'].values)

# 划分训练集和测试集
train_x, train_y = df[['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖率']], df[['好瓜']]
test_x, test_y = df[:1][['色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖率']], df[:1][['好瓜']]

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
# training
gnb.fit(train_x, train_y.values.ravel())
# predicting
pred_y = gnb.predict(test_x)
print(pred_y)

# 结果为【1】.
```

<a herf = "D:\Data_structure\Data_structure\qiuzhao秋招\示例程序\naive_bayes_GaussianNB.py" title = '示例程序'>



## 3. 集成模型 

## 5. 常见问题

### 框架一：PyTorch

* 面试问题1：pytorch 的模型的train() 方法和eval() 方法有什么区别？
  * 答： pytorch 提供两个方式来切换训练模式和评估模式。训练模式为$model.train()$ ，评估模式为$model.evel()$ 。训练模式有一个参数$mode = True \text{\\}False$ ，参数为False时即是$model.eval()$  。这两个也主要是设置了model的属性值$model.training$的区别。
  * 除此以外，$model.eval()$模式会彻底锁死模型的参数值。不然，即使不训练，模型的参数值也可能会随着输入数据的进行改变模型的参数，这主要是由于$batch\text{ }normalization$层带来的。$.eval()$模式会将$Dorpout()$以及$BN$层的权值锁死。

